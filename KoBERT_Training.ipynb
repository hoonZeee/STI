{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### KoBERTê¸°ë°˜ ì»¬ëŸ¼ì˜ˆì¸¡ CTAë¥¼ ìœ„í•œ ì½”ë“œì…ë‹ˆë‹¤"
      ],
      "metadata": {
        "id": "cmv0MM36XzZp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_XY8FsuRrkr7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6BaQDlnqW6K",
        "outputId": "2dece354-e2c6-4671-fb25-e2a5b89d1f85",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m110.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m90.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m99.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9KHi7Moc41FE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7d6c9d3-ef55-40d2-9502-3f556b6d04c4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### íŒŒì¸íŠœë‹ìš© training ì½”ë“œ"
      ],
      "metadata": {
        "id": "bGgdHu66h3ih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "from transformers import BertTokenizer, BertModel, get_scheduler\n",
        "\n",
        "# âœ… 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "df = pd.read_csv(\"train_mixed_name.csv\", encoding='utf-8-sig')\n",
        "label_map = {label: i for i, label in enumerate(df[\"label\"].unique())}\n",
        "df[\"label_id\"] = df[\"label\"].map(label_map)\n",
        "\n",
        "# âœ… 2. í† í¬ë‚˜ì´ì € ë° BERT ëª¨ë¸ ë¡œë“œ\n",
        "tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n",
        "bertmodel = BertModel.from_pretrained('monologg/kobert')\n",
        "\n",
        "# âœ… 3. ì»¤ìŠ¤í…€ Dataset ì •ì˜\n",
        "class NameDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoded = tokenizer(self.texts[idx], padding='max_length', truncation=True, max_length=32, return_tensors=\"pt\")\n",
        "        return {\n",
        "            'input_ids': encoded['input_ids'].squeeze(),\n",
        "            'attention_mask': encoded['attention_mask'].squeeze(),\n",
        "            'label': torch.tensor(self.labels[idx])\n",
        "        }\n",
        "\n",
        "dataset = NameDataset(df[\"text\"].tolist(), df[\"label_id\"].tolist())\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# âœ… 4. ë¶„ë¥˜ê¸° ëª¨ë¸ ì •ì˜\n",
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self, bert, hidden_size=768, num_classes=4):\n",
        "        super().__init__()\n",
        "        self.bert = bert\n",
        "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output  # [CLS] í† í° ì„ë² ë”©\n",
        "        return self.classifier(pooled_output)\n",
        "\n",
        "# âœ… 5. í•™ìŠµ ì¤€ë¹„\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BERTClassifier(bertmodel, num_classes=len(label_map)).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# âœ… 6. Scheduler ì„¤ì •\n",
        "num_epochs = 7\n",
        "num_training_steps = len(dataloader) * num_epochs\n",
        "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "\n",
        "# âœ… 7. í•™ìŠµ ë£¨í”„\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"âœ… Epoch {epoch+1} ì™„ë£Œ - í‰ê·  Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# âœ… 8. ëª¨ë¸ ì €ì¥\n",
        "torch.save(model.state_dict(), \"kobert_name_finetuned.pt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKpL7ydZh6VK",
        "outputId": "f8144b01-ac6d-4351-f874-7e42847e3098"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n",
            "The class this function is called from is 'BertTokenizer'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Epoch 1 ì™„ë£Œ - í‰ê·  Loss: 0.8601\n",
            "âœ… Epoch 2 ì™„ë£Œ - í‰ê·  Loss: 0.5106\n",
            "âœ… Epoch 3 ì™„ë£Œ - í‰ê·  Loss: 0.3266\n",
            "âœ… Epoch 4 ì™„ë£Œ - í‰ê·  Loss: 0.3110\n",
            "âœ… Epoch 5 ì™„ë£Œ - í‰ê·  Loss: 0.3059\n",
            "âœ… Epoch 6 ì™„ë£Œ - í‰ê·  Loss: 0.3646\n",
            "âœ… Epoch 7 ì™„ë£Œ - í‰ê·  Loss: 0.3579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ì»¬ëŸ¼ ì˜ˆì¸¡"
      ],
      "metadata": {
        "id": "rfJ4_6-7s6rc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# âœ… í›ˆë ¨ ì‹œ ì‚¬ìš©í•œ ë¼ë²¨ ë§µ ê³ ì •\n",
        "label_map = {\"ì„±ëª…\": 0, \"ì¶œìƒ\": 1, \"ì„±ë³„\": 2, \"ì§€ì—­\": 3}\n",
        "\n",
        "# âœ… ëª¨ë¸ ì •ì˜\n",
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self, bert, hidden_size=768, num_classes=4):\n",
        "        super().__init__()\n",
        "        self.bert = bert\n",
        "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        return self.classifier(pooled_output)\n",
        "\n",
        "# âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
        "tokenizer = BertTokenizer.from_pretrained(\"monologg/kobert\")\n",
        "bertmodel = BertModel.from_pretrained(\"monologg/kobert\")\n",
        "model = BERTClassifier(bertmodel, num_classes=4)\n",
        "model.load_state_dict(torch.load(\"kobert_name_finetuned.pt\", map_location=\"cpu\"))\n",
        "model.eval()\n",
        "\n",
        "# âœ… ì„±ëª… ì˜ˆì¸¡ í•¨ìˆ˜\n",
        "def predict_is_name(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=32)\n",
        "    with torch.no_grad():\n",
        "        logits = model(inputs['input_ids'], inputs['attention_mask'])\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        return probs[0][label_map[\"ì„±ëª…\"]].item()\n",
        "\n",
        "# âœ… íŒê²°ë¬¸ CSV ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "df = pd.read_csv(\"judgement.csv\", encoding='cp949')\n",
        "\n",
        "# âœ… ì»¬ëŸ¼ë³„ í‰ê·  ì„±ëª…í™•ë¥  ê³„ì‚°\n",
        "results = []\n",
        "for col in df.columns:\n",
        "    values = df[col].dropna().astype(str).tolist()[:100]\n",
        "    probs = [predict_is_name(val) for val in values]\n",
        "    avg_prob = sum(probs) / len(probs) if probs else 0\n",
        "    results.append((col, avg_prob))\n",
        "\n",
        "# âœ… ì¶œë ¥\n",
        "results.sort(key=lambda x: x[1], reverse=True)\n",
        "print(\"ğŸ§  CSVì—ì„œ 'ì„±ëª…' ì»¬ëŸ¼ìœ¼ë¡œ ê°€ì¥ ìœ ë ¥í•œ í›„ë³´:\")\n",
        "for col, score in results:\n",
        "    print(f\" {col}: ì„±ëª…ì¼ í™•ë¥  í‰ê·  {score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffT8BwwGN6-K",
        "outputId": "a6d005fa-8756-4f33-91ce-09eb4de86c74"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n",
            "The class this function is called from is 'BertTokenizer'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§  CSVì—ì„œ 'ì„±ëª…' ì»¬ëŸ¼ìœ¼ë¡œ ê°€ì¥ ìœ ë ¥í•œ í›„ë³´:\n",
            " ì´ë¦„_ë³„ëª…: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.9995\n",
            " ì£¼ë¬¸: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.1504\n",
            " ìƒì‚°ë…„ë„: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n",
            " ë§ˆì´í¬ë¡œí•„ë¦„ë²ˆí˜¸: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n",
            " íŒê²°ë‚ ì§œ: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n",
            " ì—°ë²ˆ: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n",
            " ë³¸ì ì£¼ì†Œ: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n",
            " íŒê²°ë¬¸_ë²ˆì—­ë³¸_ì œê³µ: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n",
            " íŒê²°ë¬¸_ì›ë¬¸_ì œê³µ: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n",
            " ê´€ë¦¬ë²ˆí˜¸: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n",
            " ë‹¹ì‹œë‚˜ì´: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n",
            " ì‚¬ê±´ê°œìš”: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n",
            " ì£„ëª…: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# âœ… í›ˆë ¨ ì‹œ ì‚¬ìš©í•œ ë¼ë²¨ ë§µ ê³ ì •\n",
        "label_map = {\"ì„±ëª…\": 0, \"ì¶œìƒ\": 1, \"ì„±ë³„\": 2, \"ì£¼ì†Œ\": 3}\n",
        "\n",
        "# âœ… ëª¨ë¸ ì •ì˜\n",
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self, bert, hidden_size=768, num_classes=4):\n",
        "        super().__init__()\n",
        "        self.bert = bert\n",
        "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        return self.classifier(pooled_output)\n",
        "\n",
        "# âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
        "tokenizer = BertTokenizer.from_pretrained(\"monologg/kobert\")\n",
        "bertmodel = BertModel.from_pretrained(\"monologg/kobert\")\n",
        "model = BERTClassifier(bertmodel, num_classes=4)\n",
        "model.load_state_dict(torch.load(\"kobert_name_finetuned.pt\", map_location=\"cpu\"))\n",
        "model.eval()\n",
        "\n",
        "# âœ… ì„±ëª… ì˜ˆì¸¡ í•¨ìˆ˜\n",
        "def predict_is_name(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=32)\n",
        "    with torch.no_grad():\n",
        "        logits = model(inputs['input_ids'], inputs['attention_mask'])\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        return probs[0][label_map[\"ì„±ëª…\"]].item()\n",
        "\n",
        "# âœ… íŒê²°ë¬¸ CSV ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "df = pd.read_csv(\"judgement2.csv\", encoding='cp949')\n",
        "\n",
        "# âœ… ì»¬ëŸ¼ë³„ í‰ê·  ì„±ëª…í™•ë¥  ê³„ì‚°\n",
        "results = []\n",
        "for col in df.columns:\n",
        "    values = df[col].dropna().astype(str).tolist()[:100]\n",
        "    probs = [predict_is_name(val) for val in values]\n",
        "    avg_prob = sum(probs) / len(probs) if probs else 0\n",
        "    results.append((col, avg_prob))\n",
        "\n",
        "# âœ… ì¶œë ¥\n",
        "results.sort(key=lambda x: x[1], reverse=True)\n",
        "print(\"ğŸ§  CSVì—ì„œ 'ì„±ëª…' ì»¬ëŸ¼ìœ¼ë¡œ ê°€ì¥ ìœ ë ¥í•œ í›„ë³´:\")\n",
        "for col, score in results:\n",
        "    print(f\" {col}: ì„±ëª…ì¼ í™•ë¥  í‰ê·  {score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziDM0ZRpUJlO",
        "outputId": "06b0cba3-29c5-4954-ce29-be67d23767cb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n",
            "The class this function is called from is 'BertTokenizer'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§  CSVì—ì„œ 'ì„±ëª…' ì»¬ëŸ¼ìœ¼ë¡œ ê°€ì¥ ìœ ë ¥í•œ í›„ë³´:\n",
            " ì´ë¦„_ì´ë¦„: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.9995\n",
            " ì£¼ë¬¸: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.1504\n",
            " ìƒì‚°ë…„ë„: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n",
            " ë§ˆì´í¬ë¡œí•„ë¦„ë²ˆí˜¸: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n",
            " íŒê²°ë‚ ì§œ: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n",
            " ì—°ë²ˆ: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n",
            " ë³¸ì ì£¼ì†Œ: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n",
            " íŒê²°ë¬¸_ë²ˆì—­ë³¸_ì œê³µ: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n",
            " íŒê²°ë¬¸_ì›ë¬¸_ì œê³µ: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n",
            " ê´€ë¦¬ë²ˆí˜¸: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n",
            " ë‹¹ì‹œë‚˜ì´: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n",
            " ì‚¬ê±´ê°œìš”: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n",
            " ì£„ëª…: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n"
          ]
        }
      ]
    }
  ]
}