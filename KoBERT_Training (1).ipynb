{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### KoBERTê¸°ë°˜ ì»¬ëŸ¼ì˜ˆì¸¡ CTAë¥¼ ìœ„í•œ ì½”ë“œì…ë‹ˆë‹¤"
      ],
      "metadata": {
        "id": "cmv0MM36XzZp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "í•™ìŠµìš© í…ìŠ¤íŠ¸ ë°ì´í„° ë§Œë“¤ê¸°\n",
        "- ê° ì…€ì˜ ê°’ì„ textë¡œ ì €ì¥í•˜ê³ , í•´ë‹¹ ì…€ì˜ ì»¬ëŸ¼ëª…ì„ labelë¡œ ì €ì¥\n",
        "\n",
        "ì „ì²˜ë¦¬\n",
        "- ê²°ì¸¡ì¹˜ ì œê±°, íŠ¹ìˆ˜ë¬¸ì ì´ìƒì¹˜ ì œê±°\n",
        "- ê¸¸ì´ ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì œê±° (ex. 1ê¸€ì ì´í•˜)\n",
        "- í•„ìš”ì‹œ strip(), lower() ë“± í…ìŠ¤íŠ¸ ì •ë¦¬\n",
        "\n",
        "ì‚¬ìš© ì´ìœ \n",
        "-  \"ì„±ëª…\", \"ì¶œìƒ\", \"ì„±ë³„\", \"ì§€ì—­\"ê³¼ ê°™ì€ ì •í˜•ì  ê°œë…ì„ êµ¬ë¶„í•´ì•¼ í•˜ë¯€ë¡œ\n",
        "â†’ ì‚¬ì „í•™ìŠµëœ ì–¸ì–´ëª¨ë¸ì¸ KoBERTë¥¼ ê¸°ë°˜ìœ¼ë¡œ â†’ í…ìŠ¤íŠ¸ â†’ ì˜ë¯¸(Label) ë¶„ë¥˜ ë¬¸ì œë¡œ ì„¤ê³„í–ˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "í•™ìŠµ ë°©ì‹\n",
        "- ì „ì²´ êµ¬ì¡°ëŠ” KoBERT + Linear Layer + Softmax + CrossEntropy Loss ì…ë‹ˆë‹¤\n",
        "ì¦‰, [CLS] í† í°ì—ì„œ ë‚˜ì˜¨ ë¬¸ì¥ ì„ë² ë”©ì„ ë°›ì•„\n",
        "â†’ ì„ í˜• ë¶„ë¥˜ê¸°ë¡œ í•„ìš”í•œë§Œí¼ í´ë˜ìŠ¤ë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.\n",
        "\n",
        "- batch_size, learning_rate, epoch, scheduler ë“±ì€ ì‹¤í—˜ì ìœ¼ë¡œ ì¡°ì •í•˜ë©° í•™ìŠµì´ ê³¼ì í•©ë˜ì§€ ì•Šë„ë¡ ì¡°ì ˆí•©ë‹ˆë‹¤.\n",
        "\n",
        "ì˜ˆì¸¡ë°©ì‹\n",
        "- í•™ìŠµëœ ëª¨ë¸ì€ ë¬¸ì¥ì„ ì…ë ¥ë°›ì•„ [1, num_classes] í˜•íƒœì˜ logitsë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
        "ì´ logitsì— softmaxë¥¼ ì ìš©í•´ í´ë˜ìŠ¤ë³„ í™•ë¥ ì„ ì–»ê³ , argmax()ë¡œ ì˜ˆì¸¡ ë¼ë²¨ì„ ë½‘ê±°ë‚˜ íŠ¹ì • ë¼ë²¨(ì„±ëª…, ì§€ì—­)ì¼ í™•ë¥ ë§Œ ì¶”ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "ì‹¤í—˜ê²°ê³¼í•´ì„\n",
        "- ì„±ëª… í•™ìŠµ ë°ì´í„°ê°€ ì¶©ë¶„í–ˆê¸° ë•Œë¬¸ì—,\n",
        "\"ì„±ëª…\" ì»¬ëŸ¼ì„ êµ¬ë³„í•˜ëŠ” ì •í™•ë„ê°€ ë§¤ìš° ë†’ê²Œ ë‚˜ì™”ìŠµë‹ˆë‹¤.\n",
        "(ì˜ˆ: ì´ë¦„_ë³„ëª… ì»¬ëŸ¼ â†’ ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.999)\n",
        "- ë™ì¼ êµ¬ì¡°ë¡œ \"ì§€ì—­\", \"ì¶œìƒ\" ë“±ë„ í•™ìŠµë°ì´í„°ë§Œ í™•ë³´ë˜ë©´\n",
        "STI ì „ì²´ ìë™ ë¶„ë¥˜ê¸°ë¡œ í™•ì¥ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "\n",
        "ì¥ì \n",
        "- ê¸°ì¡´ Rule-based ë°©ì‹ê³¼ ë‹¬ë¦¬ ë¬¸ì¥ ë‚´ ì˜ë¯¸ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ íŒŒì•…í•  ìˆ˜ ìˆìŒ\n",
        "- í•™ìŠµëœ ë¶„ë¥˜ê¸°ë¥¼ í†µí•´ ë‹¤ì–‘í•œ í…Œì´ë¸”ì—ë„ ì¼ë°˜í™” ê°€ëŠ¥\n",
        "- ìƒˆë¡œìš´ ê°œë…ì„ ì¶”ê°€í•˜ê³  ì‹¶ì„ ë•ŒëŠ” ë¼ë²¨ë§Œ í™•ì¥í•˜ë©´ ë¨\n",
        "- ì˜ˆì¸¡ê°’ì„ í™•ë¥ ë¡œ ë°›ì•„ ì‹ ë¢°ë„ ê¸°ë°˜ í•„í„°ë§ë„ ê°€ëŠ¥\n",
        "\n",
        "ê²°ë¡ \n",
        "- ë³¸ ëª¨ë¸ì€ í…ìŠ¤íŠ¸ ê¸°ë°˜ ì»¬ëŸ¼ ì˜ë¯¸ ë¶„ë¥˜ê¸°ë¡œ ë™ì‘í•˜ë©°,\n",
        "ì‚¬ì „í•™ìŠµëœ KoBERT ëª¨ë¸ì˜ í‘œí˜„ë ¥ê³¼ ê°„ë‹¨í•œ Linear Classifier êµ¬ì¡°ë¥¼ ê²°í•©í•˜ì—¬\n",
        "STI(semantic table interpretation) ì‘ì—…ì„ íš¨ìœ¨ì ìœ¼ë¡œ ìë™í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "f2PWZqAyZe3y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_XY8FsuRrkr7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6BaQDlnqW6K",
        "outputId": "e509e142-b03b-4c7b-e3ad-35aef7bcd853",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/install.py\", line 447, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/install.py\", line 578, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n",
            "    dependencies = list(dist.iter_dependencies())\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/metadata/importlib/_dists.py\", line 225, in iter_dependencies\n",
            "    elif not extras and req.marker.evaluate({\"extra\": \"\"}):\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/packaging/markers.py\", line 325, in evaluate\n",
            "    return _evaluate_markers(self._markers, current_environment)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/packaging/markers.py\", line 224, in _evaluate_markers\n",
            "    lhs_value, rhs_value = _normalize(lhs_value, rhs_value, key=environment_key)\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/packaging/markers.py\", line 198, in _normalize\n",
            "    return tuple(canonicalize_name(v) for v in values)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/packaging/markers.py\", line 198, in <genexpr>\n",
            "    return tuple(canonicalize_name(v) for v in values)\n",
            "\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
            "    return command.main(cmd_args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
            "    return self._main(args)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
            "    return run(options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 215, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1536, in critical\n",
            "    self._log(CRITICAL, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1622, in _log\n",
            "    fn, lno, func, sinfo = self.findCaller(stack_info, stacklevel)\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1582, in findCaller\n",
            "    if not _is_internal_frame(f):\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 194, in _is_internal_frame\n",
            "    def _is_internal_frame(frame):\n",
            "    \n",
            "KeyboardInterrupt\n",
            "^C\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/install.py\", line 447, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/install.py\", line 578, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/check.py\", line 43, in create_package_set_from_installed\n",
            "    package_set[name] = PackageDetails(dist.version, dependencies)\n",
            "                                       ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/metadata/importlib/_dists.py\", line 175, in version\n",
            "    return parse_version(self._dist.version)\n",
            "                         ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/metadata/__init__.py\", line 632, in version\n",
            "    return self.metadata['Version']\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/metadata/__init__.py\", line 617, in metadata\n",
            "    return _adapters.Message(email.message_from_string(text))\n",
            "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/email/__init__.py\", line 37, in message_from_string\n",
            "    return Parser(*args, **kws).parsestr(s)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/email/parser.py\", line 67, in parsestr\n",
            "    return self.parse(StringIO(text), headersonly=headersonly)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/email/parser.py\", line 56, in parse\n",
            "    feedparser.feed(data)\n",
            "  File \"/usr/lib/python3.11/email/feedparser.py\", line 173, in feed\n",
            "    self._input.push(data)\n",
            "  File \"/usr/lib/python3.11/email/feedparser.py\", line 109, in push\n",
            "    parts = self._partial.readlines()\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
            "    return command.main(cmd_args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
            "    return self._main(args)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
            "    return run(options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 215, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1526, in critical\n",
            "    def critical(self, msg, *args, **kwargs):\n",
            "\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9KHi7Moc41FE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### íŒŒì¸íŠœë‹ìš© training ì½”ë“œ"
      ],
      "metadata": {
        "id": "bGgdHu66h3ih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "from transformers import BertTokenizer, BertModel, get_scheduler\n",
        "\n",
        "# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "df = pd.read_csv(\"train_mixed_name.csv\", encoding='utf-8-sig')\n",
        "label_map = {label: i for i, label in enumerate(df[\"label\"].unique())}\n",
        "df[\"label_id\"] = df[\"label\"].map(label_map)\n",
        "\n",
        "# 2. í† í¬ë‚˜ì´ì € ë° BERT ëª¨ë¸ ë¡œë“œ\n",
        "tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n",
        "bertmodel = BertModel.from_pretrained('monologg/kobert')\n",
        "\n",
        "# 3. ì»¤ìŠ¤í…€ Dataset ì •ì˜\n",
        "class NameDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoded = tokenizer(self.texts[idx], padding='max_length', truncation=True, max_length=32, return_tensors=\"pt\")\n",
        "        return {\n",
        "            'input_ids': encoded['input_ids'].squeeze(),\n",
        "            'attention_mask': encoded['attention_mask'].squeeze(),\n",
        "            'label': torch.tensor(self.labels[idx])\n",
        "        }\n",
        "\n",
        "dataset = NameDataset(df[\"text\"].tolist(), df[\"label_id\"].tolist())\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# 4. ë¶„ë¥˜ê¸° ëª¨ë¸ ì •ì˜\n",
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self, bert, hidden_size=768, num_classes=4):\n",
        "        super().__init__()\n",
        "        self.bert = bert\n",
        "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output  # [CLS] í† í° ì„ë² ë”©\n",
        "        return self.classifier(pooled_output)\n",
        "\n",
        "# 5. í•™ìŠµ ì¤€ë¹„\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BERTClassifier(bertmodel, num_classes=len(label_map)).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# 6. Scheduler ì„¤ì •\n",
        "num_epochs = 7\n",
        "num_training_steps = len(dataloader) * num_epochs\n",
        "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "\n",
        "# 7. í•™ìŠµ ë£¨í”„\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"âœ… Epoch {epoch+1} ì™„ë£Œ - í‰ê·  Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# 8. ëª¨ë¸ ì €ì¥\n",
        "torch.save(model.state_dict(), \"kobert_name_finetuned.pt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKpL7ydZh6VK",
        "outputId": "f8144b01-ac6d-4351-f874-7e42847e3098"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n",
            "The class this function is called from is 'BertTokenizer'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Epoch 1 ì™„ë£Œ - í‰ê·  Loss: 0.8601\n",
            "âœ… Epoch 2 ì™„ë£Œ - í‰ê·  Loss: 0.5106\n",
            "âœ… Epoch 3 ì™„ë£Œ - í‰ê·  Loss: 0.3266\n",
            "âœ… Epoch 4 ì™„ë£Œ - í‰ê·  Loss: 0.3110\n",
            "âœ… Epoch 5 ì™„ë£Œ - í‰ê·  Loss: 0.3059\n",
            "âœ… Epoch 6 ì™„ë£Œ - í‰ê·  Loss: 0.3646\n",
            "âœ… Epoch 7 ì™„ë£Œ - í‰ê·  Loss: 0.3579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ì»¬ëŸ¼ ì˜ˆì¸¡"
      ],
      "metadata": {
        "id": "rfJ4_6-7s6rc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# í›ˆë ¨ ì‹œ ì‚¬ìš©í•œ ë¼ë²¨ ë§µ ê³ ì •\n",
        "label_map = {\"ì„±ëª…\": 0, \"ì¶œìƒ\": 1, \"ì„±ë³„\": 2, \"ì§€ì—­\": 3}\n",
        "\n",
        "# ëª¨ë¸ ì •ì˜\n",
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self, bert, hidden_size=768, num_classes=4):\n",
        "        super().__init__()\n",
        "        self.bert = bert\n",
        "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        return self.classifier(pooled_output)\n",
        "\n",
        "# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
        "tokenizer = BertTokenizer.from_pretrained(\"monologg/kobert\")\n",
        "bertmodel = BertModel.from_pretrained(\"monologg/kobert\")\n",
        "model = BERTClassifier(bertmodel, num_classes=4)\n",
        "model.load_state_dict(torch.load(\"kobert_name_finetuned.pt\", map_location=\"cpu\"))\n",
        "model.eval()\n",
        "\n",
        "# ì„±ëª… ì˜ˆì¸¡ í•¨ìˆ˜\n",
        "def predict_is_name(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=32)\n",
        "    with torch.no_grad():\n",
        "        logits = model(inputs['input_ids'], inputs['attention_mask'])\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        return probs[0][label_map[\"ì„±ëª…\"]].item()\n",
        "\n",
        "# íŒê²°ë¬¸ CSV ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "df = pd.read_csv(\"judgement.csv\", encoding='cp949')\n",
        "\n",
        "# ì»¬ëŸ¼ë³„ í‰ê·  ì„±ëª…í™•ë¥  ê³„ì‚°\n",
        "results = []\n",
        "for col in df.columns:\n",
        "    values = df[col].dropna().astype(str).tolist()[:100]\n",
        "    probs = [predict_is_name(val) for val in values]\n",
        "    avg_prob = sum(probs) / len(probs) if probs else 0\n",
        "    results.append((col, avg_prob))\n",
        "\n",
        "# ì¶œë ¥\n",
        "results.sort(key=lambda x: x[1], reverse=True)\n",
        "print(\"ğŸ§  CSVì—ì„œ 'ì„±ëª…' ì»¬ëŸ¼ìœ¼ë¡œ ê°€ì¥ ìœ ë ¥í•œ í›„ë³´:\")\n",
        "for col, score in results:\n",
        "    print(f\" {col}: ì„±ëª…ì¼ í™•ë¥  í‰ê·  {score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffT8BwwGN6-K",
        "outputId": "a6d005fa-8756-4f33-91ce-09eb4de86c74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n",
            "The class this function is called from is 'BertTokenizer'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§  CSVì—ì„œ 'ì„±ëª…' ì»¬ëŸ¼ìœ¼ë¡œ ê°€ì¥ ìœ ë ¥í•œ í›„ë³´:\n",
            " ì´ë¦„_ë³„ëª…: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.9995\n",
            " ì£¼ë¬¸: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.1504\n",
            " ìƒì‚°ë…„ë„: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n",
            " ë§ˆì´í¬ë¡œí•„ë¦„ë²ˆí˜¸: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n",
            " íŒê²°ë‚ ì§œ: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n",
            " ì—°ë²ˆ: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n",
            " ë³¸ì ì£¼ì†Œ: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n",
            " íŒê²°ë¬¸_ë²ˆì—­ë³¸_ì œê³µ: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n",
            " íŒê²°ë¬¸_ì›ë¬¸_ì œê³µ: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n",
            " ê´€ë¦¬ë²ˆí˜¸: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n",
            " ë‹¹ì‹œë‚˜ì´: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n",
            " ì‚¬ê±´ê°œìš”: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n",
            " ì£„ëª…: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# í›ˆë ¨ ì‹œ ì‚¬ìš©í•œ ë¼ë²¨ ë§µ ê³ ì •\n",
        "label_map = {\"ì„±ëª…\": 0, \"ì¶œìƒ\": 1, \"ì„±ë³„\": 2, \"ì£¼ì†Œ\": 3}\n",
        "\n",
        "# ëª¨ë¸ ì •ì˜\n",
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self, bert, hidden_size=768, num_classes=4):\n",
        "        super().__init__()\n",
        "        self.bert = bert\n",
        "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        return self.classifier(pooled_output)\n",
        "\n",
        "# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
        "tokenizer = BertTokenizer.from_pretrained(\"monologg/kobert\")\n",
        "bertmodel = BertModel.from_pretrained(\"monologg/kobert\")\n",
        "model = BERTClassifier(bertmodel, num_classes=4)\n",
        "model.load_state_dict(torch.load(\"kobert_name_finetuned.pt\", map_location=\"cpu\"))\n",
        "model.eval()\n",
        "\n",
        "# ì„±ëª… ì˜ˆì¸¡ í•¨ìˆ˜\n",
        "def predict_is_name(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=32)\n",
        "    with torch.no_grad():\n",
        "        logits = model(inputs['input_ids'], inputs['attention_mask'])\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        return probs[0][label_map[\"ì„±ëª…\"]].item()\n",
        "\n",
        "# íŒê²°ë¬¸ CSV ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "df = pd.read_csv(\"judgement2.csv\", encoding='cp949')\n",
        "\n",
        "# ì»¬ëŸ¼ë³„ í‰ê·  ì„±ëª…í™•ë¥  ê³„ì‚°\n",
        "results = []\n",
        "for col in df.columns:\n",
        "    values = df[col].dropna().astype(str).tolist()[:100]\n",
        "    probs = [predict_is_name(val) for val in values]\n",
        "    avg_prob = sum(probs) / len(probs) if probs else 0\n",
        "    results.append((col, avg_prob))\n",
        "\n",
        "# ì¶œë ¥\n",
        "results.sort(key=lambda x: x[1], reverse=True)\n",
        "print(\"ğŸ§  CSVì—ì„œ 'ì„±ëª…' ì»¬ëŸ¼ìœ¼ë¡œ ê°€ì¥ ìœ ë ¥í•œ í›„ë³´:\")\n",
        "for col, score in results:\n",
        "    print(f\" {col}: ì„±ëª…ì¼ í™•ë¥  í‰ê·  {score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziDM0ZRpUJlO",
        "outputId": "06b0cba3-29c5-4954-ce29-be67d23767cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n",
            "The class this function is called from is 'BertTokenizer'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§  CSVì—ì„œ 'ì„±ëª…' ì»¬ëŸ¼ìœ¼ë¡œ ê°€ì¥ ìœ ë ¥í•œ í›„ë³´:\n",
            " ì´ë¦„_ì´ë¦„: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.9995\n",
            " ì£¼ë¬¸: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.1504\n",
            " ìƒì‚°ë…„ë„: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n",
            " ë§ˆì´í¬ë¡œí•„ë¦„ë²ˆí˜¸: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n",
            " íŒê²°ë‚ ì§œ: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n",
            " ì—°ë²ˆ: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n",
            " ë³¸ì ì£¼ì†Œ: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n",
            " íŒê²°ë¬¸_ë²ˆì—­ë³¸_ì œê³µ: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n",
            " íŒê²°ë¬¸_ì›ë¬¸_ì œê³µ: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n",
            " ê´€ë¦¬ë²ˆí˜¸: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n",
            " ë‹¹ì‹œë‚˜ì´: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n",
            " ì‚¬ê±´ê°œìš”: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n",
            " ì£„ëª…: ì„±ëª…ì¼ í™•ë¥  í‰ê·  0.0008\n"
          ]
        }
      ]
    }
  ]
}