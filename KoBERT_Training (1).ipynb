{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### KoBERT기반 컬럼예측 CTA를 위한 코드입니다"
      ],
      "metadata": {
        "id": "cmv0MM36XzZp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습용 텍스트 데이터 만들기\n",
        "- 각 셀의 값을 text로 저장하고, 해당 셀의 컬럼명을 label로 저장\n",
        "\n",
        "전처리\n",
        "- 결측치 제거, 특수문자 이상치 제거\n",
        "- 길이 너무 짧은 텍스트 제거 (ex. 1글자 이하)\n",
        "- 필요시 strip(), lower() 등 텍스트 정리\n",
        "\n",
        "사용 이유\n",
        "-  \"성명\", \"출생\", \"성별\", \"지역\"과 같은 정형적 개념을 구분해야 하므로\n",
        "→ 사전학습된 언어모델인 KoBERT를 기반으로 → 텍스트 → 의미(Label) 분류 문제로 설계했습니다.\n",
        "\n",
        "학습 방식\n",
        "- 전체 구조는 KoBERT + Linear Layer + Softmax + CrossEntropy Loss 입니다\n",
        "즉, [CLS] 토큰에서 나온 문장 임베딩을 받아\n",
        "→ 선형 분류기로 필요한만큼 클래스로 매핑합니다.\n",
        "\n",
        "- batch_size, learning_rate, epoch, scheduler 등은 실험적으로 조정하며 학습이 과적합되지 않도록 조절합니다.\n",
        "\n",
        "예측방식\n",
        "- 학습된 모델은 문장을 입력받아 [1, num_classes] 형태의 logits를 출력합니다.\n",
        "이 logits에 softmax를 적용해 클래스별 확률을 얻고, argmax()로 예측 라벨을 뽑거나 특정 라벨(성명, 지역)일 확률만 추출할 수 있습니다.\n",
        "\n",
        "실험결과해석\n",
        "- 성명 학습 데이터가 충분했기 때문에,\n",
        "\"성명\" 컬럼을 구별하는 정확도가 매우 높게 나왔습니다.\n",
        "(예: 이름_별명 컬럼 → 성명일 확률 평균 0.999)\n",
        "- 동일 구조로 \"지역\", \"출생\" 등도 학습데이터만 확보되면\n",
        "STI 전체 자동 분류기로 확장이 가능합니다.\n",
        "\n",
        "\n",
        "\n",
        "장점\n",
        "- 기존 Rule-based 방식과 달리 문장 내 의미를 자연스럽게 파악할 수 있음\n",
        "- 학습된 분류기를 통해 다양한 테이블에도 일반화 가능\n",
        "- 새로운 개념을 추가하고 싶을 때는 라벨만 확장하면 됨\n",
        "- 예측값을 확률로 받아 신뢰도 기반 필터링도 가능\n",
        "\n",
        "결론\n",
        "- 본 모델은 텍스트 기반 컬럼 의미 분류기로 동작하며,\n",
        "사전학습된 KoBERT 모델의 표현력과 간단한 Linear Classifier 구조를 결합하여\n",
        "STI(semantic table interpretation) 작업을 효율적으로 자동화할 수 있습니다."
      ],
      "metadata": {
        "id": "f2PWZqAyZe3y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_XY8FsuRrkr7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6BaQDlnqW6K",
        "outputId": "e509e142-b03b-4c7b-e3ad-35aef7bcd853",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/install.py\", line 447, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/install.py\", line 578, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n",
            "    dependencies = list(dist.iter_dependencies())\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/metadata/importlib/_dists.py\", line 225, in iter_dependencies\n",
            "    elif not extras and req.marker.evaluate({\"extra\": \"\"}):\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/packaging/markers.py\", line 325, in evaluate\n",
            "    return _evaluate_markers(self._markers, current_environment)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/packaging/markers.py\", line 224, in _evaluate_markers\n",
            "    lhs_value, rhs_value = _normalize(lhs_value, rhs_value, key=environment_key)\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/packaging/markers.py\", line 198, in _normalize\n",
            "    return tuple(canonicalize_name(v) for v in values)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/packaging/markers.py\", line 198, in <genexpr>\n",
            "    return tuple(canonicalize_name(v) for v in values)\n",
            "\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
            "    return command.main(cmd_args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
            "    return self._main(args)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
            "    return run(options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 215, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1536, in critical\n",
            "    self._log(CRITICAL, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1622, in _log\n",
            "    fn, lno, func, sinfo = self.findCaller(stack_info, stacklevel)\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1582, in findCaller\n",
            "    if not _is_internal_frame(f):\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 194, in _is_internal_frame\n",
            "    def _is_internal_frame(frame):\n",
            "    \n",
            "KeyboardInterrupt\n",
            "^C\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/install.py\", line 447, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/install.py\", line 578, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/check.py\", line 43, in create_package_set_from_installed\n",
            "    package_set[name] = PackageDetails(dist.version, dependencies)\n",
            "                                       ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/metadata/importlib/_dists.py\", line 175, in version\n",
            "    return parse_version(self._dist.version)\n",
            "                         ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/metadata/__init__.py\", line 632, in version\n",
            "    return self.metadata['Version']\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/metadata/__init__.py\", line 617, in metadata\n",
            "    return _adapters.Message(email.message_from_string(text))\n",
            "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/email/__init__.py\", line 37, in message_from_string\n",
            "    return Parser(*args, **kws).parsestr(s)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/email/parser.py\", line 67, in parsestr\n",
            "    return self.parse(StringIO(text), headersonly=headersonly)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/email/parser.py\", line 56, in parse\n",
            "    feedparser.feed(data)\n",
            "  File \"/usr/lib/python3.11/email/feedparser.py\", line 173, in feed\n",
            "    self._input.push(data)\n",
            "  File \"/usr/lib/python3.11/email/feedparser.py\", line 109, in push\n",
            "    parts = self._partial.readlines()\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
            "    return command.main(cmd_args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
            "    return self._main(args)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
            "    return run(options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 215, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1526, in critical\n",
            "    def critical(self, msg, *args, **kwargs):\n",
            "\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9KHi7Moc41FE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 파인튜닝용 training 코드"
      ],
      "metadata": {
        "id": "bGgdHu66h3ih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "from transformers import BertTokenizer, BertModel, get_scheduler\n",
        "\n",
        "# 1. 데이터 불러오기\n",
        "df = pd.read_csv(\"train_mixed_name.csv\", encoding='utf-8-sig')\n",
        "label_map = {label: i for i, label in enumerate(df[\"label\"].unique())}\n",
        "df[\"label_id\"] = df[\"label\"].map(label_map)\n",
        "\n",
        "# 2. 토크나이저 및 BERT 모델 로드\n",
        "tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n",
        "bertmodel = BertModel.from_pretrained('monologg/kobert')\n",
        "\n",
        "# 3. 커스텀 Dataset 정의\n",
        "class NameDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoded = tokenizer(self.texts[idx], padding='max_length', truncation=True, max_length=32, return_tensors=\"pt\")\n",
        "        return {\n",
        "            'input_ids': encoded['input_ids'].squeeze(),\n",
        "            'attention_mask': encoded['attention_mask'].squeeze(),\n",
        "            'label': torch.tensor(self.labels[idx])\n",
        "        }\n",
        "\n",
        "dataset = NameDataset(df[\"text\"].tolist(), df[\"label_id\"].tolist())\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# 4. 분류기 모델 정의\n",
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self, bert, hidden_size=768, num_classes=4):\n",
        "        super().__init__()\n",
        "        self.bert = bert\n",
        "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output  # [CLS] 토큰 임베딩\n",
        "        return self.classifier(pooled_output)\n",
        "\n",
        "# 5. 학습 준비\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BERTClassifier(bertmodel, num_classes=len(label_map)).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# 6. Scheduler 설정\n",
        "num_epochs = 7\n",
        "num_training_steps = len(dataloader) * num_epochs\n",
        "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "\n",
        "# 7. 학습 루프\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"✅ Epoch {epoch+1} 완료 - 평균 Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# 8. 모델 저장\n",
        "torch.save(model.state_dict(), \"kobert_name_finetuned.pt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKpL7ydZh6VK",
        "outputId": "f8144b01-ac6d-4351-f874-7e42847e3098"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n",
            "The class this function is called from is 'BertTokenizer'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 1 완료 - 평균 Loss: 0.8601\n",
            "✅ Epoch 2 완료 - 평균 Loss: 0.5106\n",
            "✅ Epoch 3 완료 - 평균 Loss: 0.3266\n",
            "✅ Epoch 4 완료 - 평균 Loss: 0.3110\n",
            "✅ Epoch 5 완료 - 평균 Loss: 0.3059\n",
            "✅ Epoch 6 완료 - 평균 Loss: 0.3646\n",
            "✅ Epoch 7 완료 - 평균 Loss: 0.3579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 컬럼 예측"
      ],
      "metadata": {
        "id": "rfJ4_6-7s6rc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 훈련 시 사용한 라벨 맵 고정\n",
        "label_map = {\"성명\": 0, \"출생\": 1, \"성별\": 2, \"지역\": 3}\n",
        "\n",
        "# 모델 정의\n",
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self, bert, hidden_size=768, num_classes=4):\n",
        "        super().__init__()\n",
        "        self.bert = bert\n",
        "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        return self.classifier(pooled_output)\n",
        "\n",
        "# 모델 및 토크나이저 로드\n",
        "tokenizer = BertTokenizer.from_pretrained(\"monologg/kobert\")\n",
        "bertmodel = BertModel.from_pretrained(\"monologg/kobert\")\n",
        "model = BERTClassifier(bertmodel, num_classes=4)\n",
        "model.load_state_dict(torch.load(\"kobert_name_finetuned.pt\", map_location=\"cpu\"))\n",
        "model.eval()\n",
        "\n",
        "# 성명 예측 함수\n",
        "def predict_is_name(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=32)\n",
        "    with torch.no_grad():\n",
        "        logits = model(inputs['input_ids'], inputs['attention_mask'])\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        return probs[0][label_map[\"성명\"]].item()\n",
        "\n",
        "# 판결문 CSV 불러오기\n",
        "df = pd.read_csv(\"judgement.csv\", encoding='cp949')\n",
        "\n",
        "# 컬럼별 평균 성명확률 계산\n",
        "results = []\n",
        "for col in df.columns:\n",
        "    values = df[col].dropna().astype(str).tolist()[:100]\n",
        "    probs = [predict_is_name(val) for val in values]\n",
        "    avg_prob = sum(probs) / len(probs) if probs else 0\n",
        "    results.append((col, avg_prob))\n",
        "\n",
        "# 출력\n",
        "results.sort(key=lambda x: x[1], reverse=True)\n",
        "print(\"🧠 CSV에서 '성명' 컬럼으로 가장 유력한 후보:\")\n",
        "for col, score in results:\n",
        "    print(f\" {col}: 성명일 확률 평균 {score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffT8BwwGN6-K",
        "outputId": "a6d005fa-8756-4f33-91ce-09eb4de86c74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n",
            "The class this function is called from is 'BertTokenizer'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧠 CSV에서 '성명' 컬럼으로 가장 유력한 후보:\n",
            " 이름_별명: 성명일 확률 평균 0.9995\n",
            " 주문: 성명일 확률 평균 0.1504\n",
            " 생산년도: 성명일 확률 평균 0.0008\n",
            " 마이크로필름번호: 성명일 확률 평균 0.0008\n",
            " 판결날짜: 성명일 확률 평균 0.0008\n",
            " 연번: 성명일 확률 평균 0.0008\n",
            " 본적주소: 성명일 확률 평균 0.0008\n",
            " 판결문_번역본_제공: 성명일 확률 평균 0.0008\n",
            " 판결문_원문_제공: 성명일 확률 평균 0.0008\n",
            " 관리번호: 성명일 확률 평균 0.0008\n",
            " 당시나이: 성명일 확률 평균 0.0008\n",
            " 사건개요: 성명일 확률 평균 0.0008\n",
            " 죄명: 성명일 확률 평균 0.0008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 훈련 시 사용한 라벨 맵 고정\n",
        "label_map = {\"성명\": 0, \"출생\": 1, \"성별\": 2, \"주소\": 3}\n",
        "\n",
        "# 모델 정의\n",
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self, bert, hidden_size=768, num_classes=4):\n",
        "        super().__init__()\n",
        "        self.bert = bert\n",
        "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        return self.classifier(pooled_output)\n",
        "\n",
        "# 모델 및 토크나이저 로드\n",
        "tokenizer = BertTokenizer.from_pretrained(\"monologg/kobert\")\n",
        "bertmodel = BertModel.from_pretrained(\"monologg/kobert\")\n",
        "model = BERTClassifier(bertmodel, num_classes=4)\n",
        "model.load_state_dict(torch.load(\"kobert_name_finetuned.pt\", map_location=\"cpu\"))\n",
        "model.eval()\n",
        "\n",
        "# 성명 예측 함수\n",
        "def predict_is_name(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=32)\n",
        "    with torch.no_grad():\n",
        "        logits = model(inputs['input_ids'], inputs['attention_mask'])\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        return probs[0][label_map[\"성명\"]].item()\n",
        "\n",
        "# 판결문 CSV 불러오기\n",
        "df = pd.read_csv(\"judgement2.csv\", encoding='cp949')\n",
        "\n",
        "# 컬럼별 평균 성명확률 계산\n",
        "results = []\n",
        "for col in df.columns:\n",
        "    values = df[col].dropna().astype(str).tolist()[:100]\n",
        "    probs = [predict_is_name(val) for val in values]\n",
        "    avg_prob = sum(probs) / len(probs) if probs else 0\n",
        "    results.append((col, avg_prob))\n",
        "\n",
        "# 출력\n",
        "results.sort(key=lambda x: x[1], reverse=True)\n",
        "print(\"🧠 CSV에서 '성명' 컬럼으로 가장 유력한 후보:\")\n",
        "for col, score in results:\n",
        "    print(f\" {col}: 성명일 확률 평균 {score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziDM0ZRpUJlO",
        "outputId": "06b0cba3-29c5-4954-ce29-be67d23767cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n",
            "The class this function is called from is 'BertTokenizer'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧠 CSV에서 '성명' 컬럼으로 가장 유력한 후보:\n",
            " 이름_이름: 성명일 확률 평균 0.9995\n",
            " 주문: 성명일 확률 평균 0.1504\n",
            " 생산년도: 성명일 확률 평균 0.0008\n",
            " 마이크로필름번호: 성명일 확률 평균 0.0008\n",
            " 판결날짜: 성명일 확률 평균 0.0008\n",
            " 연번: 성명일 확률 평균 0.0008\n",
            " 본적주소: 성명일 확률 평균 0.0008\n",
            " 판결문_번역본_제공: 성명일 확률 평균 0.0008\n",
            " 판결문_원문_제공: 성명일 확률 평균 0.0008\n",
            " 관리번호: 성명일 확률 평균 0.0008\n",
            " 당시나이: 성명일 확률 평균 0.0008\n",
            " 사건개요: 성명일 확률 평균 0.0008\n",
            " 죄명: 성명일 확률 평균 0.0008\n"
          ]
        }
      ]
    }
  ]
}